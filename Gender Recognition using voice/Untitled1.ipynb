{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b208cdeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\dell\\anaconda3\\lib\\site-packages (1.23.5)\n",
      "Requirement already satisfied: pandas in c:\\users\\dell\\anaconda3\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\dell\\anaconda3\\lib\\site-packages (4.64.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\dell\\anaconda3\\lib\\site-packages (1.2.1)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\dell\\anaconda3\\lib\\site-packages (2.15.0)\n",
      "Requirement already satisfied: pyaudio in c:\\users\\dell\\anaconda3\\lib\\site-packages (0.2.14)\n",
      "Requirement already satisfied: librosa in c:\\users\\dell\\anaconda3\\lib\\site-packages (0.10.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scikit-learn) (1.1.1)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scikit-learn) (1.10.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.15.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.60.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.23.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (22.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (65.6.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.4.0)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: soxr>=0.3.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from librosa) (0.3.7)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: pooch>=1.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from librosa) (1.4.0)\n",
      "Requirement already satisfied: audioread>=2.1.9 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: msgpack>=1.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from librosa) (1.0.3)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from librosa) (0.12.1)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from librosa) (0.3)\n",
      "Requirement already satisfied: numba>=0.51.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from librosa) (0.56.4)\n",
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from numba>=0.51.0->librosa) (0.39.1)\n",
      "Requirement already satisfied: requests in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pooch>=1.0->librosa) (2.28.1)\n",
      "Requirement already satisfied: appdirs in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pooch>=1.0->librosa) (1.4.4)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from soundfile>=0.12.1->librosa) (1.15.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.15.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: pycparser in c:\\users\\dell\\anaconda3\\lib\\site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.25.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests->pooch>=1.0->librosa) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests->pooch>=1.0->librosa) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests->pooch>=1.0->librosa) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests->pooch>=1.0->librosa) (2.0.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy pandas tqdm scikit-learn tensorflow pyaudio librosa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf61a225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tqdm\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "label2int = {\n",
    "    \"male\": 1,\n",
    "    \"female\": 0\n",
    "}\n",
    "\n",
    "\n",
    "def load_data(vector_length=128):\n",
    "    \"\"\"A function to load gender recognition dataset from `data` folder\n",
    "    After the second run, this will load from results/features.npy and results/labels.npy files\n",
    "    as it is much faster!\"\"\"\n",
    "    # make sure results folder exists\n",
    "    if not os.path.isdir(\"results\"):\n",
    "        os.mkdir(\"results\")\n",
    "    # if features & labels already loaded individually and bundled, load them from there instead\n",
    "    if os.path.isfile(\"results/features.npy\") and os.path.isfile(\"results/labels.npy\"):\n",
    "        X = np.load(\"results/features.npy\")\n",
    "        y = np.load(\"results/labels.npy\")\n",
    "        return X, y\n",
    "    # read dataframe\n",
    "    df = pd.read_csv(\"balanced-all.csv\")\n",
    "    # get total samples\n",
    "    n_samples = len(df)\n",
    "    # get total male samples\n",
    "    n_male_samples = len(df[df['gender'] == 'male'])\n",
    "    # get total female samples\n",
    "    n_female_samples = len(df[df['gender'] == 'female'])\n",
    "    print(\"Total samples:\", n_samples)\n",
    "    print(\"Total male samples:\", n_male_samples)\n",
    "    print(\"Total female samples:\", n_female_samples)\n",
    "    # initialize an empty array for all audio features\n",
    "    X = np.zeros((n_samples, vector_length))\n",
    "    # initialize an empty array for all audio labels (1 for male and 0 for female)\n",
    "    y = np.zeros((n_samples, 1))\n",
    "    for i, (filename, gender) in tqdm.tqdm(enumerate(zip(df['filename'], df['gender'])), \"Loading data\", total=n_samples):\n",
    "        features = np.load(filename)\n",
    "        X[i] = features\n",
    "        y[i] = label2int[gender]\n",
    "    # save the audio features and labels into files\n",
    "    # so we won't load each one of them next run\n",
    "    np.save(\"results/features\", X)\n",
    "    np.save(\"results/labels\", y)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def split_data(X, y, test_size=0.1, valid_size=0.1):\n",
    "    # split training set and testing set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=7)\n",
    "    # split training set and validation set\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=valid_size, random_state=7)\n",
    "    # return a dictionary of values\n",
    "    return {\n",
    "        \"X_train\": X_train,\n",
    "        \"X_valid\": X_valid,\n",
    "        \"X_test\": X_test,\n",
    "        \"y_train\": y_train,\n",
    "        \"y_valid\": y_valid,\n",
    "        \"y_test\": y_test\n",
    "    }\n",
    "\n",
    "\n",
    "def create_model(vector_length=128):\n",
    "    \"\"\"5 hidden dense layers from 256 units to 64, not the best model, but not bad.\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_shape=(vector_length,)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(256, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(128, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(128, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(64, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    # one output neuron with sigmoid activation function, 0 means female, 1 means male\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    # using binary crossentropy as it's male/female classification (binary)\n",
    "    model.compile(loss=\"binary_crossentropy\", metrics=[\"accuracy\"], optimizer=\"adam\")\n",
    "    # print summary of the model\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caa89c35",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               33024     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 156545 (611.50 KB)\n",
      "Trainable params: 156545 (611.50 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:From C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "848/848 [==============================] - 10s 7ms/step - loss: 0.5754 - accuracy: 0.7635 - val_loss: 0.3938 - val_accuracy: 0.8437\n",
      "Epoch 2/100\n",
      "848/848 [==============================] - 8s 9ms/step - loss: 0.4217 - accuracy: 0.8318 - val_loss: 0.3592 - val_accuracy: 0.8622\n",
      "Epoch 3/100\n",
      "848/848 [==============================] - 6s 7ms/step - loss: 0.3800 - accuracy: 0.8503 - val_loss: 0.3142 - val_accuracy: 0.8757\n",
      "Epoch 4/100\n",
      "848/848 [==============================] - 9s 11ms/step - loss: 0.3616 - accuracy: 0.8606 - val_loss: 0.3017 - val_accuracy: 0.8800\n",
      "Epoch 5/100\n",
      "848/848 [==============================] - 9s 11ms/step - loss: 0.3488 - accuracy: 0.8664 - val_loss: 0.3141 - val_accuracy: 0.8787\n",
      "Epoch 6/100\n",
      "848/848 [==============================] - 8s 9ms/step - loss: 0.3408 - accuracy: 0.8691 - val_loss: 0.2885 - val_accuracy: 0.8817\n",
      "Epoch 7/100\n",
      "848/848 [==============================] - 6s 8ms/step - loss: 0.3287 - accuracy: 0.8747 - val_loss: 0.2777 - val_accuracy: 0.8951\n",
      "Epoch 8/100\n",
      "848/848 [==============================] - 5s 6ms/step - loss: 0.3204 - accuracy: 0.8797 - val_loss: 0.2829 - val_accuracy: 0.8923\n",
      "Epoch 9/100\n",
      "848/848 [==============================] - 5s 6ms/step - loss: 0.3218 - accuracy: 0.8769 - val_loss: 0.2653 - val_accuracy: 0.8959\n",
      "Epoch 10/100\n",
      "848/848 [==============================] - 6s 7ms/step - loss: 0.3157 - accuracy: 0.8791 - val_loss: 0.2605 - val_accuracy: 0.8976\n",
      "Epoch 11/100\n",
      "848/848 [==============================] - 5s 6ms/step - loss: 0.3112 - accuracy: 0.8837 - val_loss: 0.2613 - val_accuracy: 0.9019\n",
      "Epoch 12/100\n",
      "848/848 [==============================] - 7s 8ms/step - loss: 0.2967 - accuracy: 0.8886 - val_loss: 0.2517 - val_accuracy: 0.9036\n",
      "Epoch 13/100\n",
      "848/848 [==============================] - 6s 7ms/step - loss: 0.2955 - accuracy: 0.8876 - val_loss: 0.2522 - val_accuracy: 0.9029\n",
      "Epoch 14/100\n",
      "848/848 [==============================] - 7s 9ms/step - loss: 0.2920 - accuracy: 0.8896 - val_loss: 0.2663 - val_accuracy: 0.9026\n",
      "Epoch 15/100\n",
      "848/848 [==============================] - 8s 9ms/step - loss: 0.2871 - accuracy: 0.8918 - val_loss: 0.2597 - val_accuracy: 0.9006\n",
      "Epoch 16/100\n",
      "848/848 [==============================] - 6s 7ms/step - loss: 0.2894 - accuracy: 0.8907 - val_loss: 0.2514 - val_accuracy: 0.9077\n",
      "Epoch 17/100\n",
      "848/848 [==============================] - 10s 12ms/step - loss: 0.2822 - accuracy: 0.8941 - val_loss: 0.2457 - val_accuracy: 0.9082\n",
      "Epoch 18/100\n",
      "848/848 [==============================] - 19s 22ms/step - loss: 0.2801 - accuracy: 0.8946 - val_loss: 0.2475 - val_accuracy: 0.9071\n",
      "Epoch 19/100\n",
      "848/848 [==============================] - 14s 17ms/step - loss: 0.2774 - accuracy: 0.8968 - val_loss: 0.2387 - val_accuracy: 0.9066\n",
      "Epoch 20/100\n",
      "848/848 [==============================] - 9s 10ms/step - loss: 0.2767 - accuracy: 0.8971 - val_loss: 0.2467 - val_accuracy: 0.9097\n",
      "Epoch 21/100\n",
      "848/848 [==============================] - 6s 7ms/step - loss: 0.2721 - accuracy: 0.8983 - val_loss: 0.2418 - val_accuracy: 0.9102\n",
      "Epoch 22/100\n",
      "848/848 [==============================] - 7s 9ms/step - loss: 0.2771 - accuracy: 0.8976 - val_loss: 0.2498 - val_accuracy: 0.9054\n",
      "Epoch 23/100\n",
      "848/848 [==============================] - 6s 7ms/step - loss: 0.2780 - accuracy: 0.8963 - val_loss: 0.2384 - val_accuracy: 0.9119\n",
      "Epoch 24/100\n",
      "848/848 [==============================] - 6s 7ms/step - loss: 0.2731 - accuracy: 0.8977 - val_loss: 0.2394 - val_accuracy: 0.9057\n",
      "Epoch 25/100\n",
      "848/848 [==============================] - 8s 9ms/step - loss: 0.2705 - accuracy: 0.8990 - val_loss: 0.2325 - val_accuracy: 0.9163\n",
      "Epoch 26/100\n",
      "848/848 [==============================] - 6s 8ms/step - loss: 0.2717 - accuracy: 0.8987 - val_loss: 0.2366 - val_accuracy: 0.9124\n",
      "Epoch 27/100\n",
      "848/848 [==============================] - 7s 8ms/step - loss: 0.2665 - accuracy: 0.8992 - val_loss: 0.2271 - val_accuracy: 0.9097\n",
      "Epoch 28/100\n",
      "848/848 [==============================] - 7s 8ms/step - loss: 0.2658 - accuracy: 0.9011 - val_loss: 0.2313 - val_accuracy: 0.9132\n",
      "Epoch 29/100\n",
      "848/848 [==============================] - 7s 8ms/step - loss: 0.2650 - accuracy: 0.9021 - val_loss: 0.2293 - val_accuracy: 0.9147\n",
      "Epoch 30/100\n",
      "848/848 [==============================] - 7s 8ms/step - loss: 0.2658 - accuracy: 0.9005 - val_loss: 0.2321 - val_accuracy: 0.9144\n",
      "Epoch 31/100\n",
      "848/848 [==============================] - 7s 8ms/step - loss: 0.2608 - accuracy: 0.9017 - val_loss: 0.2260 - val_accuracy: 0.9182\n",
      "Epoch 32/100\n",
      "848/848 [==============================] - 8s 9ms/step - loss: 0.2645 - accuracy: 0.9021 - val_loss: 0.2311 - val_accuracy: 0.9132\n",
      "Epoch 33/100\n",
      "848/848 [==============================] - 7s 8ms/step - loss: 0.2610 - accuracy: 0.9028 - val_loss: 0.2348 - val_accuracy: 0.9115\n",
      "Epoch 34/100\n",
      "848/848 [==============================] - 10s 12ms/step - loss: 0.2608 - accuracy: 0.9038 - val_loss: 0.2371 - val_accuracy: 0.9130\n",
      "Epoch 35/100\n",
      "848/848 [==============================] - 8s 10ms/step - loss: 0.2577 - accuracy: 0.9038 - val_loss: 0.2435 - val_accuracy: 0.9105\n",
      "Epoch 36/100\n",
      "848/848 [==============================] - 8s 9ms/step - loss: 0.2618 - accuracy: 0.9012 - val_loss: 0.2287 - val_accuracy: 0.9170\n",
      "Evaluating the model using 6694 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.2208\n",
      "Accuracy: 91.75%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "\n",
    "from utils import load_data, split_data, create_model\n",
    "\n",
    "# load the dataset\n",
    "X, y = load_data()\n",
    "# split the data into training, validation and testing sets\n",
    "data = split_data(X, y, test_size=0.1, valid_size=0.1)\n",
    "# construct the model\n",
    "model = create_model()\n",
    "\n",
    "# use tensorboard to view metrics\n",
    "tensorboard = TensorBoard(log_dir=\"logs\")\n",
    "# define early stopping to stop training after 5 epochs of not improving\n",
    "early_stopping = EarlyStopping(mode=\"min\", patience=5, restore_best_weights=True)\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "\n",
    "# train the model using the training set and validating using validation set\n",
    "model.fit(data[\"X_train\"], data[\"y_train\"], epochs=epochs, batch_size=batch_size, validation_data=(data[\"X_valid\"], data[\"y_valid\"]),\n",
    "          callbacks=[tensorboard, early_stopping])\n",
    "\n",
    "# save the model to a file\n",
    "model.save(\"results/model.h5\")\n",
    "\n",
    "# evaluating the model using the testing set\n",
    "print(f\"Evaluating the model using {len(data['X_test'])} samples...\")\n",
    "loss, accuracy = model.evaluate(data[\"X_test\"], data[\"y_test\"], verbose=0)\n",
    "print(f\"Loss: {loss:.4f}\")\n",
    "print(f\"Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b03f8c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_60 (Dense)            (None, 256)               33024     \n",
      "                                                                 \n",
      " dropout_50 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_61 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout_51 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_62 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_52 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_63 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_53 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_64 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_54 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_65 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 156545 (611.50 KB)\n",
      "Trainable params: 156545 (611.50 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "1/1 [==============================] - 0s 418ms/step\n",
      "Result: male\n",
      "Probabilities:     Male: 95.74%    Female: 4.26%\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import os\n",
    "import wave\n",
    "import librosa\n",
    "import numpy as np\n",
    "from sys import byteorder\n",
    "from array import array\n",
    "from struct import pack\n",
    "\n",
    "\n",
    "THRESHOLD = 500\n",
    "CHUNK_SIZE = 1024\n",
    "FORMAT = pyaudio.paInt16\n",
    "RATE = 16000\n",
    "\n",
    "SILENCE = 30\n",
    "\n",
    "def is_silent(snd_data):\n",
    "    \"Returns 'True' if below the 'silent' threshold\"\n",
    "    return max(snd_data) < THRESHOLD\n",
    "\n",
    "def normalize(snd_data):\n",
    "    \"Average the volume out\"\n",
    "    MAXIMUM = 16384\n",
    "    times = float(MAXIMUM)/max(abs(i) for i in snd_data)\n",
    "\n",
    "    r = array('h')\n",
    "    for i in snd_data:\n",
    "        r.append(int(i*times))\n",
    "    return r\n",
    "\n",
    "def trim(snd_data):\n",
    "    \"Trim the blank spots at the start and end\"\n",
    "    def _trim(snd_data):\n",
    "        snd_started = False\n",
    "        r = array('h')\n",
    "\n",
    "        for i in snd_data:\n",
    "            if not snd_started and abs(i)>THRESHOLD:\n",
    "                snd_started = True\n",
    "                r.append(i)\n",
    "\n",
    "            elif snd_started:\n",
    "                r.append(i)\n",
    "        return r\n",
    "\n",
    "    # Trim to the left\n",
    "    snd_data = _trim(snd_data)\n",
    "\n",
    "    # Trim to the right\n",
    "    snd_data.reverse()\n",
    "    snd_data = _trim(snd_data)\n",
    "    snd_data.reverse()\n",
    "    return snd_data\n",
    "\n",
    "def add_silence(snd_data, seconds):\n",
    "    \"Add silence to the start and end of 'snd_data' of length 'seconds' (float)\"\n",
    "    r = array('h', [0 for i in range(int(seconds*RATE))])\n",
    "    r.extend(snd_data)\n",
    "    r.extend([0 for i in range(int(seconds*RATE))])\n",
    "    return r\n",
    "\n",
    "def record():\n",
    "    \"\"\"\n",
    "    Record a word or words from the microphone and \n",
    "    return the data as an array of signed shorts.\n",
    "    Normalizes the audio, trims silence from the \n",
    "    start and end, and pads with 0.5 seconds of \n",
    "    blank sound to make sure VLC et al can play \n",
    "    it without getting chopped off.\n",
    "    \"\"\"\n",
    "    p = pyaudio.PyAudio()\n",
    "    stream = p.open(format=FORMAT, channels=1, rate=RATE,\n",
    "        input=True, output=True,\n",
    "        frames_per_buffer=CHUNK_SIZE)\n",
    "\n",
    "    num_silent = 0\n",
    "    snd_started = False\n",
    "\n",
    "    r = array('h')\n",
    "\n",
    "    while 1:\n",
    "        # little endian, signed short\n",
    "        snd_data = array('h', stream.read(CHUNK_SIZE))\n",
    "        if byteorder == 'big':\n",
    "            snd_data.byteswap()\n",
    "        r.extend(snd_data)\n",
    "\n",
    "        silent = is_silent(snd_data)\n",
    "\n",
    "        if silent and snd_started:\n",
    "            num_silent += 1\n",
    "        elif not silent and not snd_started:\n",
    "            snd_started = True\n",
    "\n",
    "        if snd_started and num_silent > SILENCE:\n",
    "            break\n",
    "\n",
    "    sample_width = p.get_sample_size(FORMAT)\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "\n",
    "    r = normalize(r)\n",
    "    r = trim(r)\n",
    "    r = add_silence(r, 0.5)\n",
    "    return sample_width, r\n",
    "\n",
    "def record_to_file(path):\n",
    "    \"Records from the microphone and outputs the resulting data to 'path'\"\n",
    "    sample_width, data = record()\n",
    "    data = pack('<' + ('h'*len(data)), *data)\n",
    "\n",
    "    wf = wave.open(path, 'wb')\n",
    "    wf.setnchannels(1)\n",
    "    wf.setsampwidth(sample_width)\n",
    "    wf.setframerate(RATE)\n",
    "    wf.writeframes(data)\n",
    "    wf.close()\n",
    "\n",
    "\n",
    "\n",
    "def extract_feature(file_name, **kwargs):\n",
    "    \"\"\"\n",
    "    Extract feature from audio file `file_name`\n",
    "        Features supported:\n",
    "            - MFCC (mfcc)\n",
    "            - Chroma (chroma)\n",
    "            - MEL Spectrogram Frequency (mel)\n",
    "            - Contrast (contrast)\n",
    "            - Tonnetz (tonnetz)\n",
    "        e.g:\n",
    "        `features = extract_feature(path, mel=True, mfcc=True)`\n",
    "    \"\"\"\n",
    "    mfcc = kwargs.get(\"mfcc\")\n",
    "    chroma = kwargs.get(\"chroma\")\n",
    "    mel = kwargs.get(\"mel\")\n",
    "    contrast = kwargs.get(\"contrast\")\n",
    "    tonnetz = kwargs.get(\"tonnetz\")\n",
    "    file_path = \"C:/Users/Dell/Downloads/gender-recognition-by-voice-master (1)/gender-recognition-by-voice-master/26-496-0005.wav\"\n",
    "    X,  sample_rate = librosa.core.load(file_path)\n",
    "\n",
    "    if chroma or contrast:\n",
    "        stft = np.abs(librosa.stft(X))\n",
    "    result = np.array([])\n",
    "    if mfcc:\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
    "        result = np.hstack((result, mfccs))\n",
    "    if chroma:\n",
    "        chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
    "        result = np.hstack((result, chroma))\n",
    "    if mel:\n",
    "        mel = np.mean(librosa.feature.melspectrogram(y=X, sr=sample_rate).T, axis=0)\n",
    "        result = np.hstack((result, mel))\n",
    "    if contrast:\n",
    "        contrast = np.mean(librosa.feature.spectral_contrast(S=stft, sr=sample_rate).T,axis=0)\n",
    "        result = np.hstack((result, contrast))\n",
    "    if tonnetz:\n",
    "        tonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(X), sr=sample_rate).T,axis=0)\n",
    "        result = np.hstack((result, tonnetz))\n",
    "    return result\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # load the saved model (after training)\n",
    "    # model = pickle.load(open(\"result/mlp_classifier.model\", \"rb\"))\n",
    "    from utils import load_data, split_data, create_model\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser(description=\"\"\"Gender recognition script, this will load the model you trained, \n",
    "                                    and perform inference on a sample you provide (either using your voice or a file)\"\"\")\n",
    "    parser.add_argument(\"-f\", \"--file\", help=\"The path to the file, preferred to be in WAV format\")\n",
    "    args = parser.parse_args()\n",
    "    file = args.file\n",
    "    # construct the model\n",
    "    model = create_model()\n",
    "    # load the saved/trained weights\n",
    "    model.load_weights(\"results/model.h5\")\n",
    "    if not file or not os.path.isfile(file):\n",
    "        # if file not provided, or it doesn't exist, use your voice\n",
    "        print(\"Please talk\")\n",
    "        # put the file name here\n",
    "        file = r\"C:\\path\\to\\save\\test.wav\"\n",
    "        # record the file (start talking)\n",
    "        record_to_file(file)\n",
    "    # extract features and reshape it\n",
    "    features = extract_feature(file, mel=True).reshape(1, -1)\n",
    "    # predict the gender!\n",
    "    male_prob = model.predict(features)[0][0]\n",
    "    female_prob = 1 - male_prob\n",
    "    gender = \"male\" if male_prob > female_prob else \"female\"\n",
    "    # show the result!\n",
    "    print(\"Result:\", gender)\n",
    "    print(f\"Probabilities:     Male: {male_prob*100:.2f}%    Female: {female_prob*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c07d461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Preprocessing balanced-all.csv\n",
      "Previously: 66938 rows\n",
      "Now: 66938 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features of balanced-all: 0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def extract_feature(file_name, **kwargs):\n",
    "    \"\"\"\n",
    "    Extract feature from audio file `file_name`\n",
    "        Features supported:\n",
    "            - MFCC (mfcc)\n",
    "            - Chroma (chroma)\n",
    "            - MEL Spectrogram Frequency (mel)\n",
    "            - Contrast (contrast)\n",
    "            - Tonnetz (tonnetz)\n",
    "        e.g:\n",
    "        `features = extract_feature(path, mel=True, mfcc=True)`\n",
    "    \"\"\"\n",
    "    mfcc = kwargs.get(\"mfcc\")\n",
    "    chroma = kwargs.get(\"chroma\")\n",
    "    mel = kwargs.get(\"mel\")\n",
    "    contrast = kwargs.get(\"contrast\")\n",
    "    tonnetz = kwargs.get(\"tonnetz\")\n",
    "    X, sample_rate = librosa.core.load(file_name)\n",
    "    if chroma or contrast:\n",
    "        stft = np.abs(librosa.stft(X))\n",
    "    result = np.array([])\n",
    "    if mfcc:\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
    "        result = np.hstack((result, mfccs))\n",
    "    if chroma:\n",
    "        chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
    "        result = np.hstack((result, chroma))\n",
    "    if mel:\n",
    "        mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
    "        result = np.hstack((result, mel))\n",
    "    if contrast:\n",
    "        contrast = np.mean(librosa.feature.spectral_contrast(S=stft, sr=sample_rate).T,axis=0)\n",
    "        result = np.hstack((result, contrast))\n",
    "    if tonnetz:\n",
    "        tonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(X), sr=sample_rate).T,axis=0)\n",
    "        result = np.hstack((result, tonnetz))\n",
    "    return result\n",
    "\n",
    "dirname = \"data\"\n",
    "\n",
    "if not os.path.isdir(dirname):\n",
    "    os.mkdir(dirname)\n",
    "\n",
    "\n",
    "csv_files = glob.glob(\"*.csv\")\n",
    "\n",
    "for j, csv_file in enumerate(csv_files):\n",
    "    print(\"[+] Preprocessing\", csv_file)\n",
    "    df = pd.read_csv(csv_file)\n",
    "    # only take filename and gender columns\n",
    "    new_df = df[[\"filename\", \"gender\"]]\n",
    "    print(\"Previously:\", len(new_df), \"rows\")\n",
    "    # take only male & female genders (i.e droping NaNs & 'other' gender)\n",
    "    new_df = new_df[np.logical_or(new_df['gender'] == 'female', new_df['gender'] == 'male')]\n",
    "    print(\"Now:\", len(new_df), \"rows\")\n",
    "    new_csv_file = os.path.join(dirname, csv_file)\n",
    "    # save new preprocessed CSV \n",
    "    new_df.to_csv(new_csv_file, index=False)\n",
    "    # get the folder name\n",
    "    folder_name, _ = csv_file.split(\".\")\n",
    "    audio_files = glob.glob(f\"{folder_name}/{folder_name}/*\")\n",
    "    all_audio_filenames = set(new_df[\"filename\"])\n",
    "    for i, audio_file in tqdm(list(enumerate(audio_files)), f\"Extracting features of {folder_name}\"):\n",
    "        splited = os.path.split(audio_file)\n",
    "        # audio_filename = os.path.join(os.path.split(splited[0])[-1], splited[-1])\n",
    "        audio_filename = f\"{os.path.split(splited[0])[-1]}/{splited[-1]}\"\n",
    "        # print(\"audio_filename:\", audio_filename)\n",
    "        if audio_filename in all_audio_filenames:\n",
    "            # print(\"Copyying\", audio_filename, \"...\")\n",
    "            src_path = f\"{folder_name}/{audio_filename}\"\n",
    "            target_path = f\"{dirname}/{audio_filename}\"\n",
    "            #create that folder if it doesn't exist\n",
    "            if not os.path.isdir(os.path.dirname(target_path)):\n",
    "                os.mkdir(os.path.dirname(target_path))\n",
    "            features = extract_feature(src_path, mel=True)\n",
    "            target_filename = target_path.split(\".\")[0]\n",
    "            np.save(target_filename, features)\n",
    "            # shutil.copyfile(src_path, target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "18134090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               33024     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 156545 (611.50 KB)\n",
      "Trainable params: 156545 (611.50 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "\n",
      "1/1 [==============================] - ETA: 0s\n",
      "1/1 [==============================] - 0s 238ms/step\n",
      "Result: male\n",
      "Probabilities:     Male: 96.85%    Female: 3.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-09 02:05:53.611507: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "WARNING:tensorflow:From C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "2023-12-09 02:06:05.550575: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE SSE2 SSE3 SSE4.1 SSE4.2 AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING:tensorflow:From C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python test.py --file \"C:/Users/Dell/Downloads/gender-recognition-by-voice-master (1)/gender-recognition-by-voice-master/27-124992-0003.wav\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c9aee3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
